{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"Tokenization is the process of breaking down \n",
    "a sequence of text into smaller units called tokens,\n",
    "which can be words, phrases, or even individual characters.\n",
    "Tokenization is often the first step in natural languages processing tasks \n",
    "such as text classification, named entity recognition, and sentiment analysis.\n",
    "The resulting tokens are typically used as input to further processing steps,\n",
    "such as vectorization, where the tokens are converted\n",
    "into numerical representations for machine learning models to use.\"\"\"\n",
    "data = corpus.split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello how are u?\n",
      "[('Hello', (0, 5)), (',', (5, 6)), ('how', (7, 10)), ('are', (11, 14)), ('you', (15, 18)), ('?', (18, 19)), ('911', (20, 23)), ('please', (23, 29))]\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "## Normalizer\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import NFD, StripAccents\n",
    "\n",
    "normalizer = normalizers.Sequence([NFD(), StripAccents()])\n",
    "\n",
    "print(normalizer.normalize_str(\"Héllò hôw are ü?\"))\n",
    "\n",
    "## Pre-tokenizer\n",
    "from tokenizers import pre_tokenizers\n",
    "from tokenizers.pre_tokenizers import Whitespace, Digits, Punctuation\n",
    "\n",
    "pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Digits(individual_digits=False), Punctuation()])\n",
    "print(pre_tokenizer.pre_tokenize_str(\"Hello, how are you? 911please\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = \"hello how are you, I am good\"\n",
    "\n",
    "letter = 'o'\n",
    "\n",
    "## count the number of times a letter appears in a string\n",
    "\n",
    "count = st.count(letter)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 74\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m decoded_text\n\u001b[1;32m     73\u001b[0m bpe \u001b[38;5;241m=\u001b[39m BPE(vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m---> 74\u001b[0m bpe\u001b[38;5;241m.\u001b[39mtrain(\u001b[43mdata\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "## Byte Pair Encoding (BPE) implementation\n",
    "\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple\n",
    "\n",
    "class BPE:\n",
    "    def __init__(self, vocab_size: int):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.bpe_codes = {chr(i): 0 for i in range(256)}\n",
    "\n",
    "\n",
    "    def _count_pairs(self, corpus: List[str]) -> dict:\n",
    "        pair_freq = defaultdict(int)\n",
    "        for line in corpus:\n",
    "            line = line.strip()\n",
    "            line += \" \"\n",
    "            for i in range(len(line) - 1):\n",
    "                pair_freq[line[i], line[i + 1]] += 1\n",
    "        return pair_freq\n",
    "\n",
    "    def train(self, corpus: List[str]):\n",
    "        \n",
    "        # Count the frequency of each pair of characters\n",
    "        pair_freq = defaultdict(int)\n",
    "        for line in corpus:\n",
    "            line = line.strip()\n",
    "            line += \" \"\n",
    "            for i in range(len(line) - 1):\n",
    "                pair_freq[line[i], line[i + 1]] += 1\n",
    "\n",
    "        # Merge the most frequent pair\n",
    "        for _ in range(self.vocab_size):\n",
    "            best_pair = max(pair_freq, key=pair_freq.get)\n",
    "            pair_freq = self._merge_pair(best_pair, pair_freq)\n",
    "            self.bpe_codes[best_pair] = len(self.bpe_codes)\n",
    "\n",
    "    def _merge_pair(self, pair: Tuple[str, str], pair_freq: dict) -> dict:\n",
    "        new_pair = \"\".join(pair)\n",
    "        new_pair_freq = defaultdict(int)\n",
    "        for key in pair_freq:\n",
    "            new_key = re.sub(\" \".join(pair), new_pair, \" \".join(key))\n",
    "            new_pair_freq[new_key] = pair_freq[key]\n",
    "        return new_pair_freq\n",
    "    \n",
    "    def encode(self, text: str) -> str:\n",
    "        text += \" \"\n",
    "        tokens = []\n",
    "        i = 0\n",
    "        while i < len(text) - 1:\n",
    "            pair = text[i], text[i + 1]\n",
    "            if pair in self.bpe_codes:\n",
    "                tokens.append(\"\".join(pair))\n",
    "                i += 2\n",
    "            else:\n",
    "                tokens.append(text[i])\n",
    "                i += 1\n",
    "        return \" \".join(tokens)\n",
    "    \n",
    "    def decode(self, text: str) -> str:\n",
    "        tokens = text.split()\n",
    "        decoded_text = \"\"\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            if i < len(tokens) - 1 and tokens[i] + tokens[i + 1] in self.bpe_codes:\n",
    "                decoded_text += self.bpe_codes[tokens[i] + tokens[i + 1]]\n",
    "                i += 2\n",
    "            else:\n",
    "                decoded_text += tokens[i]\n",
    "                i += 1\n",
    "        return decoded_text\n",
    "    \n",
    "bpe = BPE(vocab_size=10)\n",
    "bpe.train(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tokenization is the process of breaking down a sequence of text into smaller units called tokens, which can be words, phrases, or even individual characters',\n",
       " '\\nTokenization is often the first step in natural languages processing tasks such as text classification, named entity recognition, and sentiment analysis',\n",
       " '\\nThe resulting tokens are typically used as input to further processing steps, such as vectorization, where the tokens are converted into numerical representations for machine learning models to use',\n",
       " '']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = \"\"\"Tokenization is the process of breaking down a sequence of text into smaller units called tokens, which can be words, phrases, or even individual characters.\n",
    "Tokenization is often the first step in natural languages processing tasks such as text classification, named entity recognition, and sentiment analysis.\n",
    "The resulting tokens are typically used as input to further processing steps, such as vectorization, where the tokens are converted into numerical representations for machine learning models to use.\"\"\"\n",
    "data = corpus.split(\".\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tokenization is the process of breaking down a sequence of text into smaller units called tokens, which can be words, phrases, or even individual characters'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _count_pairs( corpus: List[str], pair_freq: dict, n_gram: int = 2) -> dict:\n",
    "    pair_freq = defaultdict(int)\n",
    "    for line in corpus:\n",
    "        line = line.strip()\n",
    "        print(line)\n",
    "        line += \" \"\n",
    "        for i in range(len(line) - n_gram):\n",
    "            key_str = [f\"{line[i + j]}\" for j in range(n_gram)]\n",
    "            key_str = \"\".join(key_str)\n",
    "            pair_freq[key_str] += 1\n",
    "            \n",
    "    return pair_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization is the process of breaking down a sequence of text into smaller units called tokens, which can be words, phrases, or even individual characters\n",
      "Tokenization is often the first step in natural languages processing tasks such as text classification, named entity recognition, and sentiment analysis\n",
      "The resulting tokens are typically used as input to further processing steps, such as vectorization, where the tokens are converted into numerical representations for machine learning models to use\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Tok': 2,\n",
       "             'oke': 5,\n",
       "             'ken': 5,\n",
       "             'eni': 2,\n",
       "             'niz': 2,\n",
       "             'iza': 3,\n",
       "             'zat': 3,\n",
       "             'ati': 5,\n",
       "             'tio': 6,\n",
       "             'ion': 6,\n",
       "             'on ': 2,\n",
       "             'n i': 3,\n",
       "             ' is': 2,\n",
       "             'is ': 2,\n",
       "             's t': 3,\n",
       "             ' th': 3,\n",
       "             'the': 4,\n",
       "             'he ': 4,\n",
       "             'e p': 1,\n",
       "             ' pr': 3,\n",
       "             'pro': 3,\n",
       "             'roc': 3,\n",
       "             'oce': 3,\n",
       "             'ces': 3,\n",
       "             'ess': 3,\n",
       "             'ss ': 1,\n",
       "             's o': 2,\n",
       "             ' of': 3,\n",
       "             'of ': 2,\n",
       "             'f b': 1,\n",
       "             ' br': 1,\n",
       "             'bre': 1,\n",
       "             'rea': 1,\n",
       "             'eak': 1,\n",
       "             'aki': 1,\n",
       "             'kin': 1,\n",
       "             'ing': 5,\n",
       "             'ng ': 5,\n",
       "             'g d': 1,\n",
       "             ' do': 1,\n",
       "             'dow': 1,\n",
       "             'own': 1,\n",
       "             'wn ': 1,\n",
       "             'n a': 1,\n",
       "             ' a ': 1,\n",
       "             'a s': 1,\n",
       "             ' se': 2,\n",
       "             'seq': 1,\n",
       "             'equ': 1,\n",
       "             'que': 1,\n",
       "             'uen': 1,\n",
       "             'enc': 1,\n",
       "             'nce': 1,\n",
       "             'ce ': 1,\n",
       "             'e o': 1,\n",
       "             'f t': 1,\n",
       "             ' te': 2,\n",
       "             'tex': 2,\n",
       "             'ext': 2,\n",
       "             'xt ': 2,\n",
       "             't i': 1,\n",
       "             ' in': 5,\n",
       "             'int': 2,\n",
       "             'nto': 2,\n",
       "             'to ': 4,\n",
       "             'o s': 1,\n",
       "             ' sm': 1,\n",
       "             'sma': 1,\n",
       "             'mal': 1,\n",
       "             'all': 3,\n",
       "             'lle': 2,\n",
       "             'ler': 1,\n",
       "             'er ': 2,\n",
       "             'r u': 1,\n",
       "             ' un': 1,\n",
       "             'uni': 1,\n",
       "             'nit': 2,\n",
       "             'its': 1,\n",
       "             'ts ': 1,\n",
       "             's c': 1,\n",
       "             ' ca': 2,\n",
       "             'cal': 3,\n",
       "             'led': 1,\n",
       "             'ed ': 4,\n",
       "             'd t': 1,\n",
       "             ' to': 5,\n",
       "             'tok': 3,\n",
       "             'ens': 3,\n",
       "             'ns,': 1,\n",
       "             's, ': 4,\n",
       "             ', w': 2,\n",
       "             ' wh': 2,\n",
       "             'whi': 1,\n",
       "             'hic': 1,\n",
       "             'ich': 1,\n",
       "             'ch ': 3,\n",
       "             'h c': 1,\n",
       "             'can': 1,\n",
       "             'an ': 1,\n",
       "             'n b': 1,\n",
       "             ' be': 1,\n",
       "             'be ': 1,\n",
       "             'e w': 1,\n",
       "             ' wo': 1,\n",
       "             'wor': 1,\n",
       "             'ord': 1,\n",
       "             'rds': 1,\n",
       "             'ds,': 1,\n",
       "             ', p': 1,\n",
       "             ' ph': 1,\n",
       "             'phr': 1,\n",
       "             'hra': 1,\n",
       "             'ras': 1,\n",
       "             'ase': 1,\n",
       "             'ses': 1,\n",
       "             'es,': 1,\n",
       "             ', o': 1,\n",
       "             ' or': 1,\n",
       "             'or ': 2,\n",
       "             'r e': 1,\n",
       "             ' ev': 1,\n",
       "             'eve': 1,\n",
       "             'ven': 1,\n",
       "             'en ': 2,\n",
       "             'ind': 1,\n",
       "             'ndi': 1,\n",
       "             'div': 1,\n",
       "             'ivi': 1,\n",
       "             'vid': 1,\n",
       "             'idu': 1,\n",
       "             'dua': 1,\n",
       "             'ual': 1,\n",
       "             'al ': 3,\n",
       "             'l c': 1,\n",
       "             ' ch': 1,\n",
       "             'cha': 1,\n",
       "             'har': 1,\n",
       "             'ara': 1,\n",
       "             'rac': 1,\n",
       "             'act': 1,\n",
       "             'cte': 1,\n",
       "             'ter': 1,\n",
       "             'ers': 1,\n",
       "             'oft': 1,\n",
       "             'fte': 1,\n",
       "             'ten': 1,\n",
       "             'n t': 1,\n",
       "             'e f': 1,\n",
       "             ' fi': 1,\n",
       "             'fir': 1,\n",
       "             'irs': 1,\n",
       "             'rst': 1,\n",
       "             'st ': 1,\n",
       "             't s': 1,\n",
       "             ' st': 2,\n",
       "             'ste': 2,\n",
       "             'tep': 2,\n",
       "             'ep ': 1,\n",
       "             'p i': 1,\n",
       "             'in ': 1,\n",
       "             'n n': 1,\n",
       "             ' na': 2,\n",
       "             'nat': 1,\n",
       "             'atu': 1,\n",
       "             'tur': 1,\n",
       "             'ura': 1,\n",
       "             'ral': 1,\n",
       "             'l l': 1,\n",
       "             ' la': 1,\n",
       "             'lan': 1,\n",
       "             'ang': 1,\n",
       "             'ngu': 1,\n",
       "             'gua': 1,\n",
       "             'uag': 1,\n",
       "             'age': 1,\n",
       "             'ges': 1,\n",
       "             'es ': 1,\n",
       "             's p': 1,\n",
       "             'ssi': 3,\n",
       "             'sin': 2,\n",
       "             'g t': 2,\n",
       "             ' ta': 1,\n",
       "             'tas': 1,\n",
       "             'ask': 1,\n",
       "             'sks': 1,\n",
       "             'ks ': 1,\n",
       "             's s': 1,\n",
       "             ' su': 2,\n",
       "             'suc': 2,\n",
       "             'uch': 2,\n",
       "             'h a': 2,\n",
       "             ' as': 3,\n",
       "             'as ': 3,\n",
       "             't c': 1,\n",
       "             ' cl': 1,\n",
       "             'cla': 1,\n",
       "             'las': 1,\n",
       "             'ass': 1,\n",
       "             'sif': 1,\n",
       "             'ifi': 1,\n",
       "             'fic': 1,\n",
       "             'ica': 3,\n",
       "             'cat': 1,\n",
       "             'on,': 3,\n",
       "             'n, ': 3,\n",
       "             ', n': 1,\n",
       "             'nam': 1,\n",
       "             'ame': 1,\n",
       "             'med': 1,\n",
       "             'd e': 1,\n",
       "             ' en': 1,\n",
       "             'ent': 4,\n",
       "             'nti': 2,\n",
       "             'tit': 1,\n",
       "             'ity': 1,\n",
       "             'ty ': 1,\n",
       "             'y r': 1,\n",
       "             ' re': 3,\n",
       "             'rec': 1,\n",
       "             'eco': 1,\n",
       "             'cog': 1,\n",
       "             'ogn': 1,\n",
       "             'gni': 1,\n",
       "             'iti': 1,\n",
       "             ', a': 1,\n",
       "             ' an': 2,\n",
       "             'and': 1,\n",
       "             'nd ': 1,\n",
       "             'd s': 1,\n",
       "             'sen': 2,\n",
       "             'tim': 1,\n",
       "             'ime': 1,\n",
       "             'men': 1,\n",
       "             'nt ': 1,\n",
       "             't a': 1,\n",
       "             'ana': 1,\n",
       "             'nal': 1,\n",
       "             'aly': 1,\n",
       "             'lys': 1,\n",
       "             'ysi': 1,\n",
       "             'sis': 1,\n",
       "             'The': 1,\n",
       "             'e r': 1,\n",
       "             'res': 2,\n",
       "             'esu': 1,\n",
       "             'sul': 1,\n",
       "             'ult': 1,\n",
       "             'lti': 1,\n",
       "             'tin': 1,\n",
       "             'ns ': 3,\n",
       "             's a': 2,\n",
       "             ' ar': 2,\n",
       "             'are': 2,\n",
       "             're ': 3,\n",
       "             'e t': 3,\n",
       "             ' ty': 1,\n",
       "             'typ': 1,\n",
       "             'ypi': 1,\n",
       "             'pic': 1,\n",
       "             'lly': 1,\n",
       "             'ly ': 1,\n",
       "             'y u': 1,\n",
       "             ' us': 2,\n",
       "             'use': 2,\n",
       "             'sed': 1,\n",
       "             'd a': 1,\n",
       "             's i': 1,\n",
       "             'inp': 1,\n",
       "             'npu': 1,\n",
       "             'put': 1,\n",
       "             'ut ': 1,\n",
       "             't t': 1,\n",
       "             'o f': 1,\n",
       "             ' fu': 1,\n",
       "             'fur': 1,\n",
       "             'urt': 1,\n",
       "             'rth': 1,\n",
       "             'her': 2,\n",
       "             'r p': 1,\n",
       "             'g s': 1,\n",
       "             'eps': 1,\n",
       "             'ps,': 1,\n",
       "             ', s': 1,\n",
       "             's v': 1,\n",
       "             ' ve': 1,\n",
       "             'vec': 1,\n",
       "             'ect': 1,\n",
       "             'cto': 1,\n",
       "             'tor': 1,\n",
       "             'ori': 1,\n",
       "             'riz': 1,\n",
       "             'whe': 1,\n",
       "             'ere': 1,\n",
       "             'e c': 1,\n",
       "             ' co': 1,\n",
       "             'con': 1,\n",
       "             'onv': 1,\n",
       "             'nve': 1,\n",
       "             'ver': 1,\n",
       "             'ert': 1,\n",
       "             'rte': 1,\n",
       "             'ted': 1,\n",
       "             'd i': 1,\n",
       "             'o n': 1,\n",
       "             ' nu': 1,\n",
       "             'num': 1,\n",
       "             'ume': 1,\n",
       "             'mer': 1,\n",
       "             'eri': 1,\n",
       "             'ric': 1,\n",
       "             'l r': 1,\n",
       "             'rep': 1,\n",
       "             'epr': 1,\n",
       "             'pre': 1,\n",
       "             'ese': 1,\n",
       "             'nta': 1,\n",
       "             'tat': 1,\n",
       "             'ons': 1,\n",
       "             's f': 1,\n",
       "             ' fo': 1,\n",
       "             'for': 1,\n",
       "             'r m': 1,\n",
       "             ' ma': 1,\n",
       "             'mac': 1,\n",
       "             'ach': 1,\n",
       "             'chi': 1,\n",
       "             'hin': 1,\n",
       "             'ine': 1,\n",
       "             'ne ': 1,\n",
       "             'e l': 1,\n",
       "             ' le': 1,\n",
       "             'lea': 1,\n",
       "             'ear': 1,\n",
       "             'arn': 1,\n",
       "             'rni': 1,\n",
       "             'nin': 1,\n",
       "             'g m': 1,\n",
       "             ' mo': 1,\n",
       "             'mod': 1,\n",
       "             'ode': 1,\n",
       "             'del': 1,\n",
       "             'els': 1,\n",
       "             'ls ': 1,\n",
       "             'o u': 1})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_count_pairs(data, {}, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
